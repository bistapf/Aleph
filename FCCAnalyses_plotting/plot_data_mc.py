# Script to plot data and MC from ALEPH

import ROOT
import os 
from collections import namedtuple
from array import array
import copy
import numpy
from argparse import ArgumentParser
import json

from plotting_config_stage1 import PlottingConfig

ROOT.gROOT.SetBatch()
ROOT.gStyle.SetOptTitle(0)
ROOT.gStyle.SetOptStat(0)

def addOverflowToLastBin(hist):
    # print("Adding overflow to last bin for histogram:", hist.GetName())
    overflow_index = hist.GetNbinsX()+1
    if hist.IsBinOverflow(overflow_index):
        # print ("Found overflow bin with index", overflow_index, "merging content with previous bin!")
        overflow = hist.GetBinContent(overflow_index)
        lastbin = hist.GetBinContent(overflow_index-1)
        hist.SetBinContent(overflow_index-1, overflow+lastbin)
        # print ("Added together overflow =", overflow, "and last bin content", lastbin, "in histogram of name", hist.GetName())



def is_valid_rdf(input_filepath, tree_name="events", do_chunks=False):

    #check if all chunks are ok
    if do_chunks:
        files = glob.glob(os.path.join(input_filepath, "chunk*"))
        if not files:
            print(f"No files found in {input_filepath}. Skipping.")
            return False

        for f in files:
            tf = ROOT.TFile.Open(f)
            if not tf or tf.IsZombie():
                print(f"File {f} is not a valid ROOT file. Skipping.")
                continue
            if not tf.Get(tree_name):
                print(f"Tree '{tree_name}' not found in file {f}. Skipping.")
                continue
    
    else:
        if not os.path.exists(input_filepath):
            print("Filepath ", input_filepath, "not found")
            return False 

        tf = ROOT.TFile.Open(input_filepath)

        if not tf or tf.IsZombie():
            print(f"File {input_filepath} is not a valid ROOT file. Skipping.")
            return False 

        if not tf.Get(tree_name):
            print(f"Tree '{tree_name}' not found in file {input_filepath}. Skipping.")
            return False 
    
    return True

def get_rdf(input_filepath):

    # print("Getting rdf from:", input_filepath)
    rdf = None
    if input_filepath.endswith(".root"):
        
        if not is_valid_rdf(input_filepath):
            print("Process ", input_filepath, " is empty. Skipping.")
            return None
        
        rdf = ROOT.RDataFrame("events", input_filepath)

        #catching exception DOES NOT WORK
        # try:
        #     rdf = ROOT.RDataFrame("events", input_filepath)
        # except:
        #     print("File {} appears to be empty.".format(input_filepath))
        #     return
    else:
        if not is_valid_rdf(input_filepath):
            print("Process ", input_filepath, " is empty. Skipping.")
            return None

        rdf = ROOT.RDataFrame("events", input_filepath+"/chunk*")

    if not rdf:
        print("Empty file for:", input_filepath, " Skipping.")
        return None

    # print(rdf.GetColumnNames())

    return rdf

def get_param_value_from_file(param_name, filepath):
    param_value = 1.

    inputfile = ROOT.TFile.Open(filepath, "READ")
    if not inputfile or inputfile.IsZombie():
        print(f"File {filepath} is not a valid ROOT file. Cannot read paramValue for normalisation. Skipping.")
        return param_value 
    
    if not inputfile.GetListOfKeys().Contains(param_name):
        raise KeyError(f"TParameter '{param_name}' not found in file '{filepath}'")
    
    param_value = inputfile.Get(param_name).GetVal()

    inputfile.Close()
    return param_value

def get_norm_factor(sample, filepath, norm_file, weighted=False, lumi=1., print_debug=False):

    norm_factor = -1.

    # Read the cross-section and its correction factors from the .json file
    if '.json' in norm_file:
        with open(norm_file, 'r') as dict_file:
            proct_dict = json.load(dict_file)
    else:
        raise Exception("Error in get_norm_factor: Only support .json files for the cross-section values currently!")

    # For the total number of events generated or sum of weights, read them from the file (input needs to be ntuple produced with FCCAnalyses!)
    if weighted:
        total_sow = get_param_value_from_file("SumOfWeights", filepath)
        norm_factor = lumi*proct_dict[sample]["crossSection"]*proct_dict[sample]["kfactor"]*proct_dict[sample]["matchingEfficiency"]/total_sow
    else:
        total_nevts = get_param_value_from_file("eventsProcessed", filepath)
        norm_factor = lumi*proct_dict[sample]["crossSection"]*proct_dict[sample]["kfactor"]*proct_dict[sample]["matchingEfficiency"]/total_nevts

    if print_debug:
        print(f"{sample} - built normfactor from:")
        if weighted:
            print("sum of weights = ", total_sow)
        else:
            print("total nevts = ", total_nevts)

        print("x-sec = ", proct_dict[sample]["crossSection"])
        print("k-factor = ", proct_dict[sample]["kfactor"])
        print("match-eff = ", proct_dict[sample]["matchingEfficiency"])
        print("lumi = ", lumi)
        print("norm_factor = ", norm_factor)

    if norm_factor < 0:
        raise Exception("Error in get_norm_factor: Probably missing a factor in the normalisation ...")

    return norm_factor


def get_hist_from_tree(proc_name, input_filepath, hist_var, hist_nbins, hist_xmin, hist_xmax, norm_file=None,
                       lumi=1., is_data= False, add_overflow=False, weighted=False):

    if is_data and weighted:
        raise Exception("Incompatible options in get_hist_from_tree() : is_data and weighted (MC weights) both true.")
    
    if not norm_file and not is_data:
        raise Exception("Error in get_hist_from_tree() on MC: no json file with normalisation info provided!")

    #going to need a TH1Model to fill:
    has_variable_binning = False
    if not isinstance(hist_nbins, int):
        has_variable_binning = True
        hist_binEdges = array("d", hist_nbins)
        hist_nBins = len(hist_nbins)-1
        #init the histogram with variable bin widths:
        hist_model = ROOT.RDF.TH1DModel(f"hist_{proc_name}_{hist_var}", f"hist_{proc_name}_{hist_var}", hist_nBins, hist_binEdges)

    else:
        hist_model = ROOT.RDF.TH1DModel(f"hist_{proc_name}_{hist_var}", f"hist_{proc_name}_{hist_var}", hist_nbins, hist_xmin, hist_xmax)

    # get the dataframe and fill the histogram model
    rdf = get_rdf(input_filepath)

    tmp_hist = rdf.Histo1D(hist_model, hist_var).GetValue() # DO NOT USE WEIGHTS

    if add_overflow:
        addOverflowToLastBin(tmp_hist)

    # scale MC with xsec and lumi
    if not is_data:
        norm_factor = get_norm_factor(proc_name, input_filepath, norm_file, weighted=weighted, lumi=lumi )
        tmp_hist.Scale(norm_factor)

    print("{} \t {:.2f}".format(proc_name, tmp_hist.Integral()))

    hist_to_add = copy.deepcopy(tmp_hist)
    hist_to_add.SetTitle(proc_name)
    return hist_to_add

# function to draw all the things #todo: add back support for applying extra cut?
def make_plot(plot, input_dir, data_proc, mc_processes, out_dir_base, 
              year="", sel_tag ="", lumi=1., ecm=1., norm_file=None,
              do_log_y=True, add_overflow=False, fix_ratio_range=(), weighted=False,
              out_format = ".png", store_root_file=False,   
              ):

    print("Plotting", plot.name)

    if weighted:
        print("Applying generator event weights to MC")
    
    #get data histogram
    data_hist = None
    for data_file in data_proc.sample_list:
        data_filepath = os.path.join(input_dir, f"{data_file}.root")
        print(f"Looking for data file: {data_file} in {data_filepath}")
        data_tmp_hist = get_hist_from_tree(data_file, data_filepath, plot.name, plot.nbins, plot.xmin, plot.xmax, 
                                           is_data= True, add_overflow=add_overflow) #dont need a norm file, nor weights for data

        #skip if nothing passes selection:
        if not data_tmp_hist.GetEntries():
            print("Empty histogram for:", data_file, " Skipping.")
            continue

        #set plotting properties:
        data_tmp_hist.SetTitle(data_proc.title)
        data_tmp_hist.SetLineColor(data_proc.colour_key)
        data_tmp_hist.SetMarkerStyle(ROOT.kFullCircle)

        # make summed up histogram of all samples in the process
        if not data_hist: 
            data_hist = copy.deepcopy(data_tmp_hist)
            data_hist.SetName("data_hist")
        else:
            data_hist.Add(data_tmp_hist)

    # get MC prediction
    mc_hists_list = []

    for proc in mc_processes:
        #loop over files in our mc process and get each histogram, then add them together
        mc_proc_hist = None
        for sample in mc_processes[proc].sample_list:
            filepath = os.path.join(input_dir, f"{sample}.root")
            print(f"Looking for MC sample: {sample} in {filepath}")
            
            tmp_hist = get_hist_from_tree(sample, filepath, plot.name, plot.nbins, plot.xmin, plot.xmax, norm_file=norm_file,
                                          lumi=lumi, is_data= False, add_overflow=add_overflow, weighted=weighted)

            #skip if nothing passes selection:
            if not tmp_hist.GetEntries():
                print("Empty histogram for:", sample, " Skipping.")
                continue

            #set plotting properties:
            tmp_hist.SetTitle(mc_processes[proc].title)
            tmp_hist.SetLineColor(mc_processes[proc].colour_key)
            tmp_hist.SetFillColor(mc_processes[proc].colour_key)

            # make summed up histogram of all samples in the process
            if not mc_proc_hist: 
                mc_proc_hist = copy.deepcopy(tmp_hist)
                mc_proc_hist.SetName(f"{proc}_hist")
            else:
                mc_proc_hist.Add(tmp_hist)
            
        # add the total histogram for this process to the MC stack 
        mc_hists_list.append(mc_proc_hist)
    

    #sort the MC by size:
    mc_hists_list = sorted(mc_hists_list, key=lambda hist: hist.Integral())
    mc_stack = ROOT.THStack("mc_stack","")
    for mc_hist in mc_hists_list:
        mc_stack.Add(mc_hist)

    #draw stuff:
    canvas = ROOT.TCanvas("canvas", "canvas", 800, 800) 
    #add ratio:
    pad_up = ROOT.TPad("pad_up", "pad_up", 0., 0., 1., 1.)
    pad_up.SetFillStyle(0)
    pad_up.SetBottomMargin(0.32)
    pad_up.SetTopMargin(0.03)
    pad_up.SetLeftMargin(0.13)
    pad_up.SetRightMargin(0.05)
    pad_up.SetLogy(do_log_y)
    pad_up.Draw()

    pad_low = ROOT.TPad("pad_low", "pad_low", 0., 0., 1., 1.);
    pad_low.SetFillStyle(0)
    pad_low.SetBottomMargin(0.12)
    pad_low.SetTopMargin(0.72)
    pad_low.SetLeftMargin(0.13)
    pad_low.SetRightMargin(0.05)
    pad_low.SetGrid()
    pad_low.Draw()

    #
    pad_up.cd()

    mc_stack.Draw("hist")
    data_hist.Draw("E0P same")

    #axes:
    mc_stack.SetMinimum(1.e0)
    mc_stack.SetMaximum(1.e6)
    mc_stack.GetYaxis().SetTitle("Events")
    mc_stack.GetXaxis().SetLabelSize(0)
    data_hist.GetXaxis().SetTitle(plot.label)

    #labels:
    Text = ROOT.TLatex()

    Text.SetNDC() 

    Text.SetTextAlign(12);
    Text.SetNDC(ROOT.kTRUE) 
    Text.SetTextSize(0.025) 
    Text.DrawLatex(0.17, 0.95, "#it{ALEPH data and simulation}") 
    lumi_tag = "#bf{{ #sqrt{{s}} = {:.0f} GeV, L = {:.2f} pb^{{-1}} }}".format(ecm, lumi)
    Text.DrawLatex(0.17, 0.9, lumi_tag)
    ana_tag = "#bf{{ {} , {} }}".format(year, sel_tag)
    Text.DrawLatex(0.17, 0.85, ana_tag)

    #legend
    legsize = 0.05*((len(mc_hists_list)+1)/2)
    # leg = ROOT.TLegend(0.58,0.86 - legsize,0.86,0.88)
    # leg.Draw()
    leg = ROOT.TLegend(0.6, 0.95 - legsize, 0.95, 0.95)
    for mc_hist in reversed(mc_hists_list):
        leg.AddEntry(mc_hist, mc_hist.GetTitle(), "f")
    leg.AddEntry(data_hist, data_hist.GetTitle(), "ep")

    leg.SetFillStyle( 0 )
    leg.SetBorderSize( 0 )
    leg.SetTextFont( 43 )
    leg.SetTextSize( 22 )
    leg.SetNColumns( 2 )
    leg.SetColumnSeparation(-0.05)
    leg.Draw()

    #ratio panel
    hist_ratio = data_hist.Clone()
    hist_ratio.Divide( mc_stack.GetStack().Last() )
    hist_ratio.GetYaxis().SetTitle("data/MC")
    hist_ratio.GetYaxis().SetTitleOffset(1.95)
    hist_ratio.GetYaxis().SetNdivisions(6)

    #set some fixed ratio: 
    if fix_ratio_range:
        hist_ratio.GetYaxis().SetRangeUser(fix_ratio_range[0], fix_ratio_range[1])

    pad_low.cd()
    pad_low.Update()
    hist_ratio.Draw("E0P")
    pad_low.RedrawAxis()

    canvas.RedrawAxis()
    canvas.Modified()
    canvas.Update()	

    if not os.path.exists(out_dir_base):
        os.makedirs(out_dir_base)

    if weighted:
        filename = f"hists_{plot.name}_weighted"
    else:
        filename = f"hists_{plot.name}"
    
    if do_log_y:
        filename+="_logY"
    
    if add_overflow:
        filename+="_addedOverflow"
        
    fileout = os.path.join(out_dir_base, f"{filename}{out_format}")

    canvas.SaveAs(fileout)

    #write out a root file for use in e.g. WS building if requested:
    if store_root_file:
        out_filepath = os.path.join(out_dir_base, f"histograms_{plot.name}.root")

        fileout = ROOT.TFile(out_filepath, "RECREATE")
        fileout.cd()

        data_hist.Write()

        for mc_hist in mc_hists_list:
            mc_hist.Write()
        
        # add the metadata as TParameters for book-keeping
        lumi_param = ROOT.TParameter(type(lumi).__name__)("Luminosity", lumi)
        lumi_param.Write()

        ecm_param = ROOT.TParameter(type(ecm).__name__)("Energy", ecm)
        ecm_param.Write()


        print("Wrote root file with histograms to:", out_filepath)

        fileout.Close()
     

if __name__ == "__main__":

    # loop over all plots : check the config imported for details
    for plot_name, plot_specs in PlottingConfig.plots_dict.items():
        print(plot_name, plot_specs)

        make_plot(plot_specs, PlottingConfig.inputs_path, PlottingConfig.data, PlottingConfig.mc_processes, PlottingConfig.outputs_path, 
              year=PlottingConfig.year, sel_tag =PlottingConfig.sel_tag, lumi=PlottingConfig.lumi, ecm=PlottingConfig.ecm, norm_file=PlottingConfig.norm_file,
              do_log_y=PlottingConfig.do_log_y, add_overflow=PlottingConfig.add_overflow, fix_ratio_range=PlottingConfig.ratio_range, 
              weighted=PlottingConfig.weighted, out_format=PlottingConfig.out_format, store_root_file=PlottingConfig.store_root_file
           )
    

